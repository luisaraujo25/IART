{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence - 2nd Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Firstly, we import the necessary libraries, packages and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "# Machine Learning\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve,roc_curve,roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model,preprocessing\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, roc_curve, auc\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## 1. Dataset Analysis and Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Attributes:\n",
    " \n",
    "Attribute | Type | Properties | Discrete vs Continuous |\n",
    "| - | - | - | - |\n",
    "| Distance from home | Ratio |  Distinctness, Order, Meaningful differences and Meaningful ratios | Continuous |\n",
    "| Distance from last transaction | Ratio | Distinctness, Order, Meaningful differences and Meaningful ratios | Continuous |\n",
    "| Ratio of purchased price to median purchased price | Ratio |  Distinctness, Order, Meaningful differences and Meaningful ratios | Continuous |\n",
    "| Repeat Retailer | Nominal (binary) | Distinctness | Discrete |\n",
    "| Used chip (used credit card) | Nominal (binary) | Distinctness | Discrete | \n",
    "| Used pin (used pin number) | Nominal (binary) | Distinctness | Discrete |\n",
    "| Online Order | Nominal (binary) | Distinctness | Discrete |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_csv('../files/card_transdata.csv')\n",
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "- You can now find basic statistics about our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = dataFrame.sample(5000)\n",
    "dataFrame.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "- First, we are checking if there are any duplicated data.\n",
    "- If yes, we remove duplicated objects.\n",
    "- For this specific dataset, there isn't any duplicated data as it can be watch in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(dataFrame)\n",
    "dataFrame.drop_duplicates(keep = False)\n",
    "cleanedDups = len(dataFrame)\n",
    "\n",
    "print(\"Original dataset length: \", length)\n",
    "print(\"Number of dups: \", length - cleanedDups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Secondly, we detect outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.pairplot(dataFrame, vars =[\"distance_from_home\", \"distance_from_last_transaction\", \"ratio_to_median_purchase_price\"], hue = 'fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If there are any outliers for the given dataset, we remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers\n",
    "dataFrame = dataFrame.loc[(dataFrame['fraud'] == 1) | (dataFrame['distance_from_home'] >= 10)]\n",
    "#dataFrame = fraud.loc[(dataFrame['fraud'] == 1) | (dataFrame['distance_from_last_transaction'] >= 10)]\n",
    "#dataFrame = fraud.loc[(dataFrame['fraud'] == 0) | (dataFrame['ratio_to_median_purchase_price'] >= 10)]\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sns.pairplot(dataFrame, vars =[\"distance_from_home\", \"distance_from_last_transaction\", \"ratio_to_median_purchase_price\"], hue = 'fraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The last step of data preprocessing for our problem is removing \"NA\" or null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataFrame.isna().sum())\n",
    "print(dataFrame.isnull().sum())\n",
    "dataFrame = pd.read_csv('../files/card_transdata.csv', na_values=['NA'])\n",
    "dataFrame.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "- In this step, we use a sample of our dataset since the given one is too long, reaching 1000000 objects.\n",
    "- After that, we create two new datasets: one only containing fraudlent transactions (fraud attribute = 1) and another contaning only non-fraudlent transactions (fraud attribute = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No fraud (class == 0) meaning legit transactions\n",
    "#Fraud (class == 1) meaning fraudulent transactions\n",
    "\n",
    "#Creating new data frames for No fraud and Fraud cases\n",
    "no_fraud=dataFrame[dataFrame.fraud == 0]\n",
    "fraud=dataFrame[dataFrame.fraud == 1]\n",
    "\n",
    "#Resampling the original dataset with 10,000 datapoints\n",
    "no_fraud_resample=no_fraud.sample(n=1000)\n",
    "fraud_resample=fraud.sample(n=1000)\n",
    "\n",
    "#Creating new dataset consisting of equal class occurence \n",
    "data=pd.concat([no_fraud_resample,fraud_resample],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset\n",
    "\n",
    "#Creating dataframe for target\n",
    "target=data['fraud']\n",
    "\n",
    "#Creating dataframe for features\n",
    "features=data.drop(columns=['fraud'],axis=1)\n",
    "\n",
    "\n",
    "train_features,test_features,train_labels,test_labels = train_test_split(features,target,stratify=target,shuffle=True,random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardising the dataset\n",
    "scale=MinMaxScaler().fit(train_features)\n",
    "train_features_scaled=scale.transform(train_features)\n",
    "test_features_scaled=scale.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## Algorithms and Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "\n",
    "data_all=[] #Creating empty list to store model evaluation values\n",
    "model_name = 'Random Forest'\n",
    "model = RandomForestClassifier()\n",
    "param_rf={\n",
    "    \"n_estimators\": range(2,15,2),\n",
    "    \"max_features\": range(1,7),\n",
    "    \"max_depth\": range(1,10)\n",
    "}\n",
    "\n",
    "\n",
    "random_rf = RandomizedSearchCV(estimator = model, \n",
    "                                param_distributions = param_rf, \n",
    "                                n_iter = 15, \n",
    "                                cv = 10,\n",
    "                                scoring = 'accuracy', \n",
    "                                verbose = 1, \n",
    "                                n_jobs = -1,\n",
    "                                random_state = 1)\n",
    "\n",
    "random_rf.fit(train_features, train_labels)\n",
    "\n",
    "\n",
    "model = random_rf.best_estimator_\n",
    "crossval = cross_val_score(model,\n",
    "                            train_features,\n",
    "                            train_labels,\n",
    "                            cv = 5,\n",
    "                            scoring = 'accuracy')\n",
    "\n",
    "scores = np.mean(crossval)\n",
    "\n",
    "test_pred = model.predict(test_features)\n",
    "test_recall = recall_score(test_labels, \n",
    "                            test_pred, \n",
    "                            pos_label = 1)\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, \n",
    "                                test_pred, \n",
    "                                pos_label = 1)\n",
    "test_auc = auc(fpr, tpr)\n",
    "\n",
    "endTime = time.time()\n",
    "\n",
    "print('Train acc:', round(scores * 100, 2), '%')\n",
    "print('Test acc:', round(accuracy_score(test_labels, test_pred) * 100, 2), '%')\n",
    "print('Best n_estimators:', random_rf.best_estimator_.get_params()['n_estimators'])\n",
    "print('Best max_features:', random_rf.best_estimator_.get_params()['max_features'])\n",
    "print('Best max_depth:', random_rf.best_estimator_.get_params()['max_depth'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_features)\n",
    "cm = confusion_matrix(test_labels, test_pred)\n",
    "clr = classification_report(test_labels, test_pred)\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(cm, \n",
    "            annot = True, \n",
    "            vmin = 0, \n",
    "            fmt = 'g', \n",
    "            cbar = False, \n",
    "            cmap = 'Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix:Random Forest\")\n",
    "plt.show()\n",
    "print(clr)\n",
    "\n",
    "if accuracy_score(test_labels, test_pred) - scores < 1:\n",
    "    result_model = 'Good Model'\n",
    "    data_all.append([model,\n",
    "                    scores,\n",
    "                    accuracy_score(test_labels, test_pred),\n",
    "                    test_recall, \n",
    "                    test_auc,\n",
    "                    result_model])\n",
    "else:\n",
    "    result_model = 'Bad Model'\n",
    "    data_all.append([model,\n",
    "                    scores,\n",
    "                    accuracy_score(test_labels, test_pred),\n",
    "                    test_recall,\n",
    "                    test_auc,\n",
    "                    result_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Running Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running time: \", endTime-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "data_all=[] #Creating empty list to store model evaluation values\n",
    "model_name='SVM'\n",
    "model= SVC()\n",
    "param_svm={\n",
    "    \"C\":[100,10,1,0.1],\n",
    "    \"gamma\": [10,1,0.1,0.001],\n",
    "    \"kernel\": ['rbf']\n",
    "    }\n",
    "\n",
    "random_svm=RandomizedSearchCV(estimator=model, param_distributions=param_svm, n_iter = 15 , scoring='accuracy', cv = 10, verbose=2, n_jobs=-1,random_state=43)\n",
    "random_svm.fit(train_features_scaled,train_labels)\n",
    "\n",
    "model=random_svm.best_estimator_\n",
    "crossval = cross_val_score(model,train_features_scaled,train_labels,cv=5,scoring='accuracy')\n",
    "scores = np.mean(crossval)\n",
    "\n",
    "test_pred=model.predict(test_features_scaled)\n",
    "test_recall = recall_score(test_labels, test_pred, pos_label=1)\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, test_pred, pos_label=1)\n",
    "test_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "endTime = time.time()\n",
    "\n",
    "print('Train accuracy:',round(scores*100,2),'%')\n",
    "print('Test accuracy:',round(accuracy_score(test_labels,test_pred)*100,2),'%')\n",
    "print('Best C:',random_svm.best_estimator_.get_params()['C'])\n",
    "print('Best gamma:',random_svm.best_estimator_.get_params()['gamma'])\n",
    "print('Best kernel:',random_svm.best_estimator_.get_params()['kernel']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred=model.predict(test_features_scaled)\n",
    "cm = confusion_matrix(test_labels, test_pred)\n",
    "clr = classification_report(test_labels, test_pred)\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cbar=False, cmap='Greens')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix: SVM\")\n",
    "plt.show()\n",
    "print(\"Classification Report:\\n\\n\", clr)\n",
    "\n",
    "if accuracy_score(test_labels,test_pred)-scores <1:\n",
    "    result_model='Good Model'\n",
    "    data_all.append([model_name,scores,accuracy_score(test_labels,test_pred),test_recall,test_auc,result_model])\n",
    "else:\n",
    "    result_model='Failed'\n",
    "    data_all.append([model_name,scores,accuracy_score(test_labels,test_pred),test_recall,test_auc,result_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a Function for Train and Test split - split is done on 80% train and 20% test (can be altered in test_size value). Initially done including all features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine Running Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running time: \", endTime-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "\n",
    "data_all=[] #Creating empty list to store model evaluation values\n",
    "model_name = 'Decision Tree'\n",
    "des_tree_model = DecisionTreeClassifier()\n",
    "drawing= des_tree_model.fit(train_features, train_labels)\n",
    "y_test_pred = des_tree_model.predict(test_features)\n",
    "y_train_pred = des_tree_model.predict(train_features)\n",
    "Accuracy_Test=accuracy_score(test_labels, y_test_pred) * 100\n",
    "Accuracy_Test= str(Accuracy_Test) + \" %\"\n",
    "Accuracy_Train=accuracy_score(train_labels, y_train_pred) * 100\n",
    "Accuracy_Train= str(Accuracy_Train) + \" %\"\n",
    "Train_Data_Score=des_tree_model.score(train_features, train_labels) * 100\n",
    "Train_Data_Score= str(Train_Data_Score) + \" %\"\n",
    "\n",
    "\n",
    "#Construction of confusion matrix\n",
    "confmat_train = confusion_matrix(y_train_pred, train_labels)\n",
    "confmat_test =  confusion_matrix(y_test_pred, test_labels)\n",
    "print (\"\\nConfusion matrix of Train Data\\n\", confmat_train)\n",
    "print (\"\\nConfusion matrix of Test Data\\n\", confmat_test)\n",
    "total=sum(sum(confmat_test))\n",
    "\n",
    "\n",
    "#Calculation of Specificity\n",
    "#Specificity = TN/(TN+FP)\n",
    "sp = confmat_test[1,1]/(confmat_test[1,0]+confmat_test[1,1]) * 100\n",
    "sp= str(sp) + \" %\"\n",
    "\n",
    "\n",
    "#Calculation of sensitivity\n",
    "#Sensitivity= TP/(TP+FN)\n",
    "sensi = confmat_test[0,0]/(confmat_test[0,0]+confmat_test[0,1]) * 100\n",
    "sensi= str(sensi) + \" %\"\n",
    "\n",
    "\n",
    "\n",
    "myTable = PrettyTable([\"Peformance Paramters\", \"Value\"]) \n",
    "#Insert rows\n",
    "myTable.add_row([\"Accuracy of Test Data\",Accuracy_Test]) \n",
    "myTable.add_row([\"Accuracy of Train Data\", Accuracy_Train]) \n",
    "myTable.add_row([\"Data score of Train Data\",Train_Data_Score]) \n",
    "myTable.add_row([\"Specificity of the confusion matrix\",sp]) \n",
    "myTable.add_row([\"Sensitivity of the confusion matrix\",sensi])\n",
    "\n",
    "endTime = time.time()\n",
    "\n",
    "if accuracy_score(test_labels,test_pred)-scores <1:\n",
    "    result_model='Good Model'\n",
    "    data_all.append([model_name,scores,accuracy_score(test_labels,test_pred),test_recall,test_auc,result_model])\n",
    "else:\n",
    "    result_model='Failed'\n",
    "    data_all.append([model_name,scores,accuracy_score(test_labels,test_pred),test_recall,test_auc,result_model])\n",
    "\n",
    "#print Output\n",
    "print(myTable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deision Tree Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix of the Decision Tree\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "def plot_confusionmat(cm, lab, test_or_train):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    f = ax.matshow(cm, cmap=plt.cm.turbo)\n",
    "    plt.title('Confusion Matrix of the ' + test_or_train +' Decision Tree \\n')\n",
    "    fig.colorbar(f)\n",
    "    ax.set_xticklabels([''] + lab)\n",
    "    ax.set_yticklabels([''] + lab)\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('True Values')\n",
    "    s = [['TP','FN'], ['FP', 'TN']]\n",
    "    for k in range(2):\n",
    "        for l in range(2):\n",
    "            plt.text(l,k, str(cm[k][l]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusionmat(confmat_train, [\"0\",\"1\"], \"Training\")\n",
    "plot_confusionmat(confmat_test, [\"0\",\"1\"], \"Testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[60,40])\n",
    "tree.plot_tree(des_tree_model,fontsize=20, rounded=True,impurity=True, precision=2)\n",
    "#tree.plot_tree(drawing,fontsize=20, rounded=True,impurity=True, precision=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "### Logisitic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Running Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'endTime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lu/Desktop/uni/IART/IART-proj2/scr/main.ipynb Cell 44'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lu/Desktop/uni/IART/IART-proj2/scr/main.ipynb#ch0000177?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning time: \u001b[39m\u001b[39m\"\u001b[39m, endTime\u001b[39m-\u001b[39mstartTime)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'endTime' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Running time: \", endTime-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<br>\n",
    "\n",
    "## Ensemble Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=['SVM','Logisitic Regression','Random Forest', 'Decision Tree']\n",
    "pd_scores=pd.DataFrame(data_all,columns=['model','train score','test score','test recall','test_auc','Remark'])\n",
    "print(pd_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "m='Logistic Regression'\n",
    "model= LogisticRegression(max_iter=1000)\n",
    "param_lr={\n",
    "    \"solver\": ['newton-cg','lbfgs','liblinear'],\n",
    "    \"C\":[100, 10, 1.0, 0.1, 0.01],\n",
    "    \"penalty\":['l2']\n",
    "}\n",
    "random_lr=RandomizedSearchCV(estimator=model, param_distributions=param_lr, n_iter = 15 , scoring='accuracy', cv = 10, verbose=2, n_jobs=-1,random_state=43)\n",
    "random_lr.fit(train_features_scaled,train_labels)\n",
    "model=random_lr.best_estimator_\n",
    "crossval = cross_val_score(model,train_features_scaled,train_labels,cv=5,scoring='accuracy')\n",
    "scores = np.mean(crossval)\n",
    "\n",
    "test_pred=model.predict(test_features_scaled)\n",
    "test_recall = recall_score(test_labels, test_pred, pos_label=1)\n",
    "fpr,tpr,thresholds = roc_curve(test_labels, test_pred, pos_label=1)\n",
    "test_auc = auc(fpr,tpr)\n",
    "\n",
    "endTime = time.time()\n",
    "\n",
    "print('Train acc:',round(scores*100,2),'%')\n",
    "print('Test acc:',round(accuracy_score(test_labels,test_pred)*100,2),'%')\n",
    "print('Best solver:',random_lr.best_estimator_.get_params()['solver'])\n",
    "print('Best C:',random_lr.best_estimator_.get_params()['C'])\n",
    "print('Best penalty:',random_lr.best_estimator_.get_params()['penalty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred=model.predict(test_features_scaled)\n",
    "cm = confusion_matrix(test_labels, test_pred)\n",
    "clr = classification_report(test_labels, test_pred)\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.heatmap(cm, annot=True, vmin=0, fmt='g', cbar=False, cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix:Log Reg\")\n",
    "plt.show()\n",
    "print(\"Classification Report:\\n----------------------\\n\", clr)\n",
    "\n",
    "if accuracy_score(test_labels,test_pred)-scores <1:\n",
    "    result_model='Good Model'\n",
    "    data_all.append([m,scores,accuracy_score(test_labels,test_pred),test_recall,test_auc,result_model])\n",
    "else:\n",
    "    result_model='Failed'\n",
    "    data_all.append([m,scores,accuracy_score(test_labels,test_pred),test_recall,test_auc,result_model])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
